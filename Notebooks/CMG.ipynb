{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "377fe0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "import traceback\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "76aa2318",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 output_dim,\n",
    "                 output_dim_s,\n",
    "                 d_model,\n",
    "                 num_heads,\n",
    "                 dff,\n",
    "                 max_seq_len,\n",
    "                 max_seq_len_out,\n",
    "                 num_layers,\n",
    "                 rate\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.input_shapes = max_seq_len\n",
    "        self.output_shapes = max_seq_len\n",
    "        self.input_embedding = tf.keras.layers.Dense(d_model, input_shape=(\n",
    "            max_seq_len, input_dim))\n",
    "        \n",
    "        self.positional_encoding_in = self._get_positional_encoding(\n",
    "            max_seq_len, d_model\n",
    "        )\n",
    "        \n",
    "        #enc\n",
    "        self.multihead_attentions = [tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model) \n",
    "                                     for _ in range(num_layers)]\n",
    "        self.add_norm_1_layers = [tf.keras.layers.LayerNormalization(epsilon=1e-6) \n",
    "                                  for _ in range(num_layers)]\n",
    "        self.feed_forward_layers = [self._get_feed_forward(dff=dff, d_model=d_model) \n",
    "                                     for _ in range(num_layers)]\n",
    "        self.add_norm_2_layers = [tf.keras.layers.LayerNormalization(epsilon=1e-6) \n",
    "                                  for _ in range(num_layers)]\n",
    "        #dec\n",
    "        self.output_embedding = tf.keras.layers.Dense(\n",
    "            d_model, input_shape=(\n",
    "                max_seq_len,\n",
    "                output_dim\n",
    "            )\n",
    "        )\n",
    "        self.positional_encoding_out = self._get_positional_encoding(\n",
    "            max_seq_len, d_model\n",
    "        )\n",
    "        self.multihead_attentions_dec_mask = [tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model) \n",
    "                                     for _ in range(num_layers)]\n",
    "        self.add_norm_1_layers_dec = [tf.keras.layers.LayerNormalization(epsilon=1e-6) \n",
    "                                  for _ in range(num_layers)]\n",
    "        self.multihead_attentions_dec = [tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model) \n",
    "                                     for _ in range(num_layers)]\n",
    "        self.add_norm_2_layers_dec = [tf.keras.layers.LayerNormalization(epsilon=1e-6) \n",
    "                                  for _ in range(num_layers)]\n",
    "        self.feed_forward_layers_dec = [self._get_feed_forward(dff=dff, d_model=d_model) \n",
    "                                     for _ in range(num_layers)]\n",
    "        self.add_norm_3_layers_dec = [tf.keras.layers.LayerNormalization(epsilon=1e-6) \n",
    "                                  for _ in range(num_layers)]\n",
    "        \n",
    "        self.final_layer = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=(max_seq_len,d_model)),\n",
    "            tf.keras.layers.LSTM(128),\n",
    "            tf.keras.layers.Dense(output_dim_s, activation='softmax')\n",
    "        ])\n",
    "        \n",
    "        \n",
    "        \n",
    "    def _get_positional_encoding(self, max_seq_len, d_model):\n",
    "        position = tf.range(max_seq_len, dtype=tf.float32)[:, tf.newaxis]\n",
    "        div_term = tf.exp(tf.range(0, d_model, 2, dtype=tf.float32) * -(tf.math.log(10000.0) / d_model))\n",
    "        sin = tf.math.sin(position * div_term)\n",
    "        cos = tf.math.cos(position * div_term)\n",
    "        pos_encoding = tf.concat([sin, cos], axis=-1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "    \n",
    "    def _get_feed_forward(self, dff, d_model):\n",
    "        return tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),\n",
    "            tf.keras.layers.Dense(d_model)\n",
    "        ])\n",
    "    \n",
    "    def create_attention_mask( self, seq_len):\n",
    "        mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        return mask\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        inputs, outputs = inputs\n",
    "        input_embed = self.input_embedding(inputs)\n",
    "        pos_encoding_in = self.positional_encoding_in[:, :self.input_shapes, :]\n",
    "        x = input_embed + pos_encoding_in\n",
    "        for i in range(self.num_layers):\n",
    "            multihead_out = self.multihead_attentions[i](x, x)\n",
    "            add_norm_1_out = self.add_norm_1_layers[i](x + multihead_out)\n",
    "            feed_forward_out = self.feed_forward_layers[i](add_norm_1_out)\n",
    "            add_norm_2_out = self.add_norm_2_layers[i](add_norm_1_out + feed_forward_out)\n",
    "            x = add_norm_2_out\n",
    "            \n",
    "        \n",
    "\n",
    "        attention_mask = tf.transpose(self.create_attention_mask(self.output_shapes))\n",
    "        attention_mask = tf.expand_dims(attention_mask, axis=0)\n",
    "        attention_mask = tf.expand_dims(attention_mask, axis=0)\n",
    "        \n",
    "        out_embed = self.output_embedding(outputs)\n",
    "        post_encoding_out = self.positional_encoding_out[:, :self.output_shapes, :]\n",
    "        y = out_embed + post_encoding_out\n",
    "        for i in range(self.num_layers):\n",
    "            multihead_out_dec_mask = self.multihead_attentions_dec_mask[i](\n",
    "                query = y, key = y, value = y\n",
    "                , attention_mask = attention_mask\n",
    "            )\n",
    "            add_norm_1_out_dec = self.add_norm_1_layers_dec[i](y + multihead_out_dec_mask)\n",
    "            multihead_out_dec = self.multihead_attentions_dec[i](\n",
    "                query = add_norm_1_out_dec, key =  add_norm_2_out, value = add_norm_2_out\n",
    "            )\n",
    "            add_norm_2_out_dec = self.add_norm_2_layers_dec[i]( add_norm_1_out_dec + multihead_out_dec)\n",
    "            feed_forward_out_dec = self.feed_forward_layers_dec[i](add_norm_2_out_dec)\n",
    "            add_norm_3_out_dec = self.add_norm_3_layers_dec[i](add_norm_2_out_dec + feed_forward_out_dec)\n",
    "            y = add_norm_3_out_dec\n",
    "        final = self.final_layer(y)\n",
    "        return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2ecb2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preopen market predictions\n",
    "def data_maker_pom_test (i):\n",
    "    df = pd.read_csv(i, index_col = [0])\n",
    "    df = df.iloc[df[df.timestamp.str.contains(df.timestamp.apply(lambda x: x.split(' ')[0]).unique()[-360])].iloc[0].name:]\n",
    "    df.reset_index(drop = True, inplace = True)\n",
    "    times = pd.DataFrame(df['timestamp'])\n",
    "    pl = pd.DataFrame(df['pl'])\n",
    "    dff = pd.DataFrame()\n",
    "    st = datetime.strptime(df.timestamp.iloc[ 0 ], '%Y-%m-%d %H:%M:%S') #+ timedelta(days = 1)\n",
    "    et = datetime.strptime(df.timestamp.iloc[-1 ], '%Y-%m-%d %H:%M:%S')\n",
    "    while True:\n",
    "        if st > et + timedelta(days = 1):\n",
    "            break\n",
    "        srt = datetime.strftime(st , '%Y-%m-%d %H:%M:%S')\n",
    "        tmp = df[df['timestamp'].str.contains(srt.split(' ')[0])].copy()\n",
    "        tmp.drop(columns = ['timestamp'], inplace = True)\n",
    "        lis = list(tmp.columns)\n",
    "        if len(tmp) == 0:\n",
    "            st += timedelta(days = 1)\n",
    "            continue\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(tmp)\n",
    "        tmp = scaler.transform(tmp)\n",
    "        \n",
    "        tmp = pd.DataFrame(tmp, columns = lis)\n",
    "        dff = pd.concat([\n",
    "            dff,\n",
    "            tmp\n",
    "        ])\n",
    "        del scaler\n",
    "        st += timedelta(days = 1)\n",
    "    dff.reset_index(drop = True, inplace = True)\n",
    "    dff['pl'] = pl['pl']\n",
    "    return dff,times,df\n",
    "\n",
    "def prepare_data_com_pom(df,i,times,timesteps):\n",
    "    pl = df[['pl']]\n",
    "    df['class'] = [1 if j[0] >0 else 0 for j in pl.values]\n",
    "    y = df[['class']]\n",
    "    df['pl'] = pl\n",
    "    df['class'] = y\n",
    "    col = list(df.columns)\n",
    "    tdf = pd.DataFrame(columns = ['timestamp'])\n",
    "    \n",
    "    dt = times.iloc[0].values[0]\n",
    "    dt = dt.split(' ')[0]\n",
    "    et = times.iloc[-1].values[0]\n",
    "    et = et.split(' ')[0]\n",
    "    dtt = datetime.strptime(dt, '%Y-%m-%d')\n",
    "    ett = datetime.strptime(et, '%Y-%m-%d')\n",
    "    fd = pd.DataFrame(columns = col)\n",
    "    fdd = df.copy()\n",
    "    while dtt < ett + timedelta(days = 1):\n",
    "        \n",
    "        try:\n",
    "            ind = times[times['timestamp'].str.contains(dtt.strftime('%Y-%m-%d'))].iloc[-1].name\n",
    "            tdf = pd.concat([\n",
    "                tdf,\n",
    "                pd.DataFrame(\n",
    "                    times[\n",
    "                        times['timestamp'].str.contains(\n",
    "                            dtt.strftime('%Y-%m-%d')\n",
    "                        )].iloc[-1]\n",
    "                ).T\n",
    "            ])\n",
    "        except:\n",
    "            dtt+= timedelta(days = 1)\n",
    "            continue \n",
    "        ad = pd.DataFrame(fdd.loc[ind-timesteps:ind],columns = col)\n",
    "        ad = ad[col]\n",
    "        fd = pd.concat([\n",
    "            fd,\n",
    "            ad\n",
    "        ])\n",
    "        dtt += timedelta(days = 1)\n",
    "    fd.reset_index(drop = True, inplace = True)\n",
    "    col = fd.columns\n",
    "    return fd, col,tdf\n",
    "\n",
    "def plc_pom(fd):\n",
    "    data = fd['pl']\n",
    "    range_50 = np.percentile(data, [16, 84]) \n",
    "    range_75 = np.percentile(data, [3, 97])  \n",
    "    range_100 = np.percentile(data, [0, 100])\n",
    "    fd['plc'] = fd['pl'].apply(\n",
    "        lambda x : 1 if (x <= range_50[1] and x > 0 ) else( \n",
    "            -1\n",
    "            if (x > range_50[0] and x < 0) else(\n",
    "                2\n",
    "                if (x > range_50[1] and x < range_75[1]) else(\n",
    "                    -2\n",
    "                    if (x < range_50[0] and x > range_75[0]) else (\n",
    "                        3 if (x > range_75[1]) else(\n",
    "                            -3 if (x < range_75[0]) else 0\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    return fd, (range_50,range_75,range_100)\n",
    "\n",
    "def plc_pom_4(fd):\n",
    "    data = fd['pl']\n",
    "    range_45 = np.percentile(data, [27.5, 72.5])\n",
    "    range_50 = np.percentile(data, [16, 84]) \n",
    "    range_75 = np.percentile(data, [3, 97])  \n",
    "    range_100 = np.percentile(data, [0, 100])\n",
    "    fd['plc'] = fd['pl'].apply(\n",
    "        lambda x : 1 if (x > 0 and x <= range_45[1]) else(\n",
    "            -1 if(x<0 and x > range_45[0] ) else(\n",
    "            2 if(x >= range_45[1] and x < range_50[1]) else(\n",
    "            -2 if (x <= range_45[0] and x > range_50[0]) else(\n",
    "            3 if (x >= range_50[1] and x < range_75[1]) else(\n",
    "            -3 if (x <= range_50[0] and x > range_75[0]) else(\n",
    "            4 if (x >= range_75[1]) else (\n",
    "            -4 if (x <= range_75[0]) else 0\n",
    "            )\n",
    "            )\n",
    "            )\n",
    "            )\n",
    "            )\n",
    "        )\n",
    "    ))\n",
    "    return fd, (range_50,range_75,range_100)\n",
    "\n",
    "def splitter_com(df,timesteps):\n",
    "    xx = np.empty((0,timesteps + 1,len(df.columns) - 3))\n",
    "    yy = np.empty((0,timesteps + 1 ,1))\n",
    "    pll = np.empty((0,timesteps + 1 , 1))\n",
    "    plcc = np.empty((0,timesteps + 1 , 1))\n",
    "    \n",
    "    yt = df[['class']]\n",
    "    pltt = df[['pl']]\n",
    "    pltc = df[['plc']]\n",
    "    \n",
    "    loop = len(df) / (timesteps+1)\n",
    "    for i in range(0,int(loop)):\n",
    "        x = df.loc[i*(timesteps+1): ((i+1) *(timesteps + 1 )) - 1].copy()\n",
    "        y = yt.loc[i*(timesteps+1): ((i+1) *(timesteps + 1 )) - 1]\n",
    "        pl = pltt.loc[i*(timesteps+1): ((i+1) *(timesteps + 1 )) - 1]\n",
    "        plc = pltc.loc[i*(timesteps+1): ((i+1) *(timesteps + 1 )) - 1]\n",
    "        \n",
    "        x.drop(columns = ['class','pl','plc'], inplace = True)\n",
    "        x = np.array(x).reshape(1,timesteps+1,len(x.columns))\n",
    "        y = np.array(y).reshape(1,timesteps+1,1)\n",
    "        pl = np.array(pl).reshape(1,timesteps+1,1)\n",
    "        plc = np.array(plc).reshape(1,timesteps+1,1)\n",
    "        \n",
    "        xx = np.append(xx,x,axis = 0)\n",
    "        yy = np.append(yy,y, axis = 0)\n",
    "        pll = np.append(pll,pl, axis = 0)\n",
    "        plcc = np.append(plcc,plc, axis = 0)\n",
    "    return xx,yy,pll,plcc\n",
    "\n",
    "def padder_com(x,y,pl,plc,sli,timesteps):\n",
    "    xtt = np.empty((0,timesteps+1,x.shape[2]))\n",
    "    ytt = np.empty((0,timesteps+1,y.shape[2]))\n",
    "    plttt = np.empty((0,timesteps+1,pl.shape[2]))\n",
    "    pltcc = np.empty((0,timesteps+1,plc.shape[2]))\n",
    "    xee = np.empty((0,timesteps+1,x.shape[2]))\n",
    "    yee = np.empty((0,timesteps+1,y.shape[2]))\n",
    "    plee = np.empty((0,timesteps+1,pl.shape[2]))\n",
    "    plecc = np.empty((0,timesteps+1,plc.shape[2]))\n",
    "    for i in range(0,len(x) - sli):\n",
    "        for j in range(1,timesteps+2):\n",
    "            xt = np.pad(\n",
    "                x[i][- j: ].reshape(1,j,x.shape[2]),\n",
    "                ((0, 0), (timesteps + 1 -j, 0), (0, 0)),\n",
    "                mode = 'constant',\n",
    "                constant_values=-1\n",
    "            )\n",
    "            yt = np.pad(\n",
    "                y[i][- j: ].reshape(1,j,y.shape[2]),\n",
    "                ((0, 0), (timesteps + 1-j, 0), (0, 0)),\n",
    "                mode = 'constant', \n",
    "                constant_values=-1\n",
    "            )\n",
    "            pltt = np.pad(\n",
    "                pl[i][- j: ].reshape(1,j,pl.shape[2]),\n",
    "                ((0, 0), (timesteps + 1 - j , 0), (0, 0)),\n",
    "                mode = 'constant', \n",
    "                constant_values=0\n",
    "            )\n",
    "            pltc = np.pad(\n",
    "                plc[i][- j: ].reshape(1,j,plc.shape[2]),\n",
    "                ((0, 0), (timesteps + 1 - j , 0), (0, 0)),\n",
    "                mode = 'constant', \n",
    "                constant_values=0\n",
    "            )\n",
    "            xtt = np.append(xtt,xt, axis = 0)\n",
    "            ytt = np.append(ytt,yt, axis = 0)\n",
    "            plttt = np.append(plttt,pltt, axis = 0)\n",
    "            pltcc = np.append(pltcc,pltc, axis = 0)\n",
    "    for i in range(len(x) - sli, len(x)):\n",
    "        for j in range(1,timesteps+2):\n",
    "            xt = np.pad(\n",
    "                x[i][- j: ].reshape(1,j,x.shape[2]),\n",
    "                ((0, 0), (timesteps + 1 - j, 0), (0, 0)),\n",
    "                mode = 'constant', \n",
    "                constant_values=-1\n",
    "            )\n",
    "            yt = np.pad(\n",
    "                y[i][- j: ].reshape(1,j,y.shape[2]),\n",
    "                ((0, 0), (timesteps + 1 - j, 0), (0, 0)),\n",
    "                mode = 'constant', \n",
    "                constant_values=-1\n",
    "            )\n",
    "            pltt = np.pad(\n",
    "                pl[i][- j: ].reshape(1,j,pl.shape[2]),\n",
    "                ((0, 0), (timesteps + 1 - j, 0), (0, 0)),\n",
    "                mode = 'constant', \n",
    "                constant_values=0\n",
    "            )\n",
    "            pltc = np.pad(\n",
    "                plc[i][- j: ].reshape(1,j,plc.shape[2]),\n",
    "                ((0, 0), (timesteps + 1 - j, 0), (0, 0)),\n",
    "                mode = 'constant', \n",
    "                constant_values=0\n",
    "            )\n",
    "            xee = np.append(xee,xt, axis = 0)\n",
    "            yee = np.append(yee,yt, axis = 0)\n",
    "            plee = np.append(plee,pltt, axis = 0)\n",
    "            plecc = np.append(plecc,pltc, axis = 0)\n",
    "\n",
    "    return xtt,ytt,plttt, pltcc,xee,yee,plee,plecc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a74aaf09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNXFMCG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XBANK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NIFTYQUALITY30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DJUSRD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DJUSMC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>BELMG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>DJUSAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>DJUSEE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>AMX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>EZ300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             symbol\n",
       "0           CNXFMCG\n",
       "1             XBANK\n",
       "2    NIFTYQUALITY30\n",
       "3            DJUSRD\n",
       "4            DJUSMC\n",
       "..              ...\n",
       "155           BELMG\n",
       "156          DJUSAS\n",
       "157          DJUSEE\n",
       "158             AMX\n",
       "159           EZ300\n",
       "\n",
       "[160 rows x 1 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_main = pd.read_csv('../data/market_indices.csv', index_col = [0])\n",
    "df_main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe758082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90a4ddcb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNXFMCG\n",
      "32/32 [==============================] - 2s 15ms/step\n",
      "Epoch 1/400\n",
      "32/32 [==============================] - 8s 41ms/step - loss: 1.7324 - accuracy: 0.2817\n",
      "Epoch 2/400\n",
      "32/32 [==============================] - 1s 44ms/step - loss: 1.6707 - accuracy: 0.3452\n",
      "Epoch 3/400\n",
      "32/32 [==============================] - 1s 44ms/step - loss: 1.6302 - accuracy: 0.3601\n",
      "Epoch 4/400\n",
      "32/32 [==============================] - 1s 39ms/step - loss: 1.5455 - accuracy: 0.3958\n",
      "Epoch 5/400\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 1.4238 - accuracy: 0.4603\n",
      "Epoch 6/400\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 1.3157 - accuracy: 0.5099\n",
      "Epoch 7/400\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 1.1024 - accuracy: 0.6091\n",
      "Epoch 8/400\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 0.9152 - accuracy: 0.6726\n",
      "Epoch 9/400\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 0.8434 - accuracy: 0.6935\n",
      "Epoch 10/400\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 0.6686 - accuracy: 0.7649\n",
      "Epoch 11/400\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 0.5336 - accuracy: 0.8095\n",
      "Epoch 12/400\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 0.5676 - accuracy: 0.8115\n",
      "Epoch 13/400\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 0.4629 - accuracy: 0.8532\n",
      "Epoch 14/400\n",
      "32/32 [==============================] - 1s 42ms/step - loss: 0.4335 - accuracy: 0.8581\n",
      "Epoch 15/400\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 0.3562 - accuracy: 0.8869\n",
      "Epoch 16/400\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 0.2165 - accuracy: 0.9335\n",
      "Epoch 17/400\n",
      "32/32 [==============================] - 1s 39ms/step - loss: 0.2736 - accuracy: 0.9077\n",
      "Epoch 18/400\n",
      "32/32 [==============================] - 1s 39ms/step - loss: 0.2704 - accuracy: 0.9157\n",
      "Epoch 19/400\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 0.2266 - accuracy: 0.9246\n",
      "Epoch 20/400\n",
      "32/32 [==============================] - 1s 39ms/step - loss: 0.3149 - accuracy: 0.8988\n",
      "Epoch 21/400\n",
      "32/32 [==============================] - 1s 39ms/step - loss: 0.1092 - accuracy: 0.9772\n",
      "Epoch 22/400\n",
      "32/32 [==============================] - 2s 49ms/step - loss: 0.0778 - accuracy: 0.9752\n",
      "Epoch 23/400\n",
      "32/32 [==============================] - 2s 68ms/step - loss: 0.1166 - accuracy: 0.9593\n",
      "Epoch 24/400\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.1029 - accuracy: 0.9692\n",
      "Epoch 25/400\n",
      "32/32 [==============================] - 2s 58ms/step - loss: 0.2609 - accuracy: 0.9167\n",
      "Epoch 26/400\n",
      "32/32 [==============================] - 2s 59ms/step - loss: 0.4812 - accuracy: 0.8571\n",
      "Epoch 27/400\n",
      "32/32 [==============================] - 2s 50ms/step - loss: 0.1501 - accuracy: 0.9573\n",
      "Epoch 28/400\n",
      "32/32 [==============================] - 1s 39ms/step - loss: 0.1508 - accuracy: 0.9573\n",
      "Epoch 29/400\n",
      "32/32 [==============================] - 1s 42ms/step - loss: 0.1437 - accuracy: 0.9583\n",
      "Epoch 30/400\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 0.0908 - accuracy: 0.9752\n",
      "Epoch 31/400\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 0.0675 - accuracy: 0.9802\n",
      "Epoch 32/400\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 0.0369 - accuracy: 0.9911\n",
      "Epoch 33/400\n",
      "32/32 [==============================] - 1s 36ms/step - loss: 0.0287 - accuracy: 0.9931\n",
      "Epoch 34/400\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 0.0486 - accuracy: 0.9901\n",
      "Epoch 35/400\n",
      "32/32 [==============================] - 1s 36ms/step - loss: 0.1089 - accuracy: 0.9683\n",
      "Epoch 36/400\n",
      "32/32 [==============================] - 1s 39ms/step - loss: 0.1338 - accuracy: 0.9583\n",
      "Epoch 37/400\n",
      "32/32 [==============================] - 1s 39ms/step - loss: 0.1714 - accuracy: 0.9464\n",
      "Epoch 38/400\n",
      "32/32 [==============================] - 1s 39ms/step - loss: 0.1121 - accuracy: 0.9663\n",
      "Epoch 39/400\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 0.0650 - accuracy: 0.9752\n",
      "Epoch 40/400\n",
      "32/32 [==============================] - 1s 43ms/step - loss: 0.0726 - accuracy: 0.9782\n",
      "Epoch 41/400\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 0.0524 - accuracy: 0.9831\n",
      "Epoch 42/400\n",
      "32/32 [==============================] - 1s 39ms/step - loss: 0.0372 - accuracy: 0.9881\n",
      "Epoch 43/400\n",
      "32/32 [==============================] - 1s 39ms/step - loss: 0.0233 - accuracy: 0.9891\n",
      "Epoch 44/400\n",
      "32/32 [==============================] - 1s 42ms/step - loss: 0.0291 - accuracy: 0.9931\n",
      "Epoch 45/400\n",
      "32/32 [==============================] - 1s 39ms/step - loss: 0.1013 - accuracy: 0.9673\n",
      "Epoch 46/400\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 0.0415 - accuracy: 0.9881\n",
      "Epoch 47/400\n",
      "32/32 [==============================] - 1s 39ms/step - loss: 0.0491 - accuracy: 0.9871\n",
      "Epoch 48/400\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 0.0285 - accuracy: 0.9931\n",
      "Epoch 49/400\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 0.0365 - accuracy: 0.9871\n",
      "Epoch 50/400\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 0.1236 - accuracy: 0.9603\n",
      "Epoch 51/400\n",
      "32/32 [==============================] - 1s 39ms/step - loss: 0.1081 - accuracy: 0.9603\n",
      "Epoch 52/400\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 0.1313 - accuracy: 0.9593\n",
      "Epoch 53/400\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 0.0577 - accuracy: 0.9841\n",
      "Epoch 54/400\n",
      "32/32 [==============================] - 1s 39ms/step - loss: 0.0462 - accuracy: 0.9831\n",
      "Epoch 55/400\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 0.0370 - accuracy: 0.9901\n",
      "Epoch 56/400\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 0.1038 - accuracy: 0.9643\n",
      "Epoch 57/400\n",
      "32/32 [==============================] - 1s 46ms/step - loss: 0.0991 - accuracy: 0.9673\n",
      "Epoch 58/400\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 0.0482 - accuracy: 0.9861\n",
      "Epoch 59/400\n",
      "32/32 [==============================] - 1s 39ms/step - loss: 0.0208 - accuracy: 0.9940\n",
      "Epoch 60/400\n",
      "31/32 [============================>.] - ETA: 0s - loss: 0.0119 - accuracy: 0.9990\n",
      "Desired accuracy reached. Stopping training.\n",
      "32/32 [==============================] - 1s 46ms/step - loss: 0.0117 - accuracy: 0.9990\n",
      "14/14 [==============================] - 0s 16ms/step\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import gc\n",
    "class StopAtAccuracy(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, target=0.995):\n",
    "        super().__init__()\n",
    "        self.target = target\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        acc = logs.get(\"accuracy\")  # Use \"val_accuracy\" if you're using validation accuracy\n",
    "        if acc is not None and acc >= self.target:\n",
    "            print(f\"\\nDesired accuracy reached. Stopping training.\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_mod_pom(timesteps):\n",
    "    input_dim = 162\n",
    "    output_dim = 1\n",
    "    output_dim_s = 6\n",
    "    max_seq_len = timesteps\n",
    "    max_seq_len_out = timesteps\n",
    "    model = TransformerModel(\n",
    "        input_dim = input_dim,\n",
    "        output_dim = output_dim,\n",
    "        output_dim_s = output_dim_s,\n",
    "        d_model = 64,#high 128 #64\n",
    "        num_heads = 8,#2 works fine\n",
    "        dff = 128,#200\n",
    "        max_seq_len = max_seq_len,\n",
    "        max_seq_len_out = max_seq_len_out ,\n",
    "        num_layers = 1,#high 1\n",
    "        rate = 0.1\n",
    "    )\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def run_simulation(i, timesteps, timess):\n",
    "    try:\n",
    "        sli = 108\n",
    "        sym = '../data/Database_sh_backup/' + i + '.csv'\n",
    "        dff,times,df = data_maker_pom_test (sym)        \n",
    "        fd, col , tdf = prepare_data_com_pom(dff.copy(),sym,times,timesteps)\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(fd[['pl']][:len(fd) - sli])\n",
    "        fd['pl'] = scaler.transform(fd[['pl']])\n",
    "        fd,tup = plc_pom(fd)\n",
    "        x,y,pl,plc = splitter_com(fd,timesteps)\n",
    "        xt, yt, pltt, pltc,xe, ye, ple, plec = padder_com(x,y,pl,plc,sli,timesteps)\n",
    "        otr = np.array([ [1 if i[0] == 1 else 0 ,0 if i[0] == 1 else 1]  for i in yt[:,-1,:]])\n",
    "        ote = np.array([ [1 if i[0] == 1 else 0 ,0 if i[0] == 1 else 1]  for i in ye[:,-1,:]])\n",
    "        otrc = np.array(\n",
    "        [\n",
    "            [\n",
    "                1 if i[0] == -3 else 0 ,\n",
    "                1 if i[0] == -2 else 0 ,\n",
    "                1 if i[0] == -1 else 0 ,\n",
    "                1 if i[0] == 1 else 0 ,\n",
    "                1 if i[0] == 2 else 0 ,\n",
    "                1 if i[0] == 3 else 0\n",
    "            ]  for i in pltc[:,-1,:]\n",
    "        ]\n",
    "        )\n",
    "        otec = np.array(\n",
    "        [\n",
    "            [\n",
    "                1 if i[0] == -3 else 0 ,\n",
    "                1 if i[0] == -2 else 0 ,\n",
    "                1 if i[0] == -1 else 0 ,\n",
    "                1 if i[0] == 1 else 0 ,\n",
    "                1 if i[0] == 2 else 0 ,\n",
    "                1 if i[0] == 3 else 0\n",
    "            ]  for i in plec[:,-1,:]\n",
    "        ]\n",
    "        )\n",
    "        model = get_mod_pom(timesteps)\n",
    "        yp = model.predict([xt[:, 1:, :], pltt[:, :-1, :]])\n",
    "        \n",
    "        timess = timess\n",
    "        history = model.fit(\n",
    "            [xt[:, 1:, :], pltt[:, :-1, :]], otrc[:],\n",
    "            epochs=timess,\n",
    "            callbacks=[StopAtAccuracy(target=0.995)]\n",
    "        \n",
    "        )\n",
    "        if os.path.exists('../cmg_results/CMG/record.csv'):\n",
    "            pd.DataFrame([[i,history.history['loss'][-1],history.history['accuracy'][-1]]],\n",
    "                         columns = ['sym','loss','accuracy']).to_csv('../cmg_results/CMG/record.csv')\n",
    "        else:\n",
    "            pd.DataFrame([[i,history.history['loss'][-1],history.history['accuracy'][-1]]], \n",
    "                      columns = ['sym','loss','accuracy']).to_csv('../cmg_results/CMG/record.csv', header = False, mode = 'a')\n",
    "            \n",
    "        yp = model.predict([xe[:, 1:, :], ple[:, :-1, :]])\n",
    "        ypp = yp\n",
    "        model.save_weights(\"../cmg_results/CMG/models/\"+i+'.h5')\n",
    "        del model\n",
    "        gc.collect()\n",
    "        count = 0\n",
    "        li = []\n",
    "        for ii in yp:\n",
    "            if (count+1)%(timesteps + 1) == 0:\n",
    "                if (ii[0] + ii[1]  +ii[2] )  >( ii[3] +ii[4]  + ii[5]):\n",
    "                    li.append(0)\n",
    "                else:\n",
    "                    li.append(1)\n",
    "            count += 1\n",
    "\n",
    "        y_pred = pd.DataFrame(li, columns = ['pred'])\n",
    "        y_pred['timestamp'] = list(list ( tdf[-int(len(yp)/(timesteps + 1)):]['timestamp'].apply(lambda x:x.split(' ')[0])))\n",
    "        y_pred = y_pred [['timestamp', 'pred']]\n",
    "        \n",
    "# ================================================\n",
    "# ⚠️ OHLC Data Usage (Commented Out)\n",
    "# ================================================\n",
    "# This section of the code utilizes OHLC (Open, High, Low, Close) financial market data.\n",
    "# Due to licensing restrictions, the actual OHLC dataset used in this project cannot be\n",
    "# uploaded or distributed publicly via this repository.\n",
    "#\n",
    "# Many OHLC data sources (e.g., Yahoo Finance, Bloomberg, NSE, etc.) allow personal or\n",
    "# research use, but prohibit redistribution or public sharing of the data files.\n",
    "#\n",
    "# To reproduce results:\n",
    "# - You may use your own OHLC data.\n",
    "# - Or modify the script to fetch data from APIs (e.g., yfinance).\n",
    "#\n",
    "# =================================================\n",
    "\n",
    "#         #ori =  '../../../../../Database_indices/Database_backup/'+i+'.csv'\n",
    "#         ori = '../data/Database_backup/'\n",
    "#         ori_df = pd.read_csv(ori, index_col = [0])    \n",
    "#         dt_end = datetime.strptime(y_pred.iloc[-1]['timestamp'],'%Y-%m-%d')\n",
    "#         acc = pd.DataFrame(columns = ['date','accuracy', 'potential'])\n",
    "#         for ind , row in y_pred.iterrows():\n",
    "#             dt = datetime.strptime(row.timestamp,'%Y-%m-%d')\n",
    "#             dt_orig = dt\n",
    "#             close_previous = ori_df [ori_df.datetime.str.contains(dt_orig.strftime('%Y-%m-%d'))].iloc[-1].close\n",
    "#             checker = 0\n",
    "#             while True:\n",
    "#                 dt += timedelta(days = 1)\n",
    "#                 if dt > dt_end:\n",
    "#                     checker = 1\n",
    "#                     break\n",
    "#                 sd = ori_df[ori_df.datetime.str.contains(dt.strftime('%Y-%m-%d'))].copy()\n",
    "#                 if len(sd) > 0:\n",
    "#                     break\n",
    "#             if checker == 1:\n",
    "#                 continue\n",
    "#             if len(sd) > 0:\n",
    "#                 sdd = sd[:int(len(sd) / 4)] #first quarter\n",
    "#                 check = 0\n",
    "#                 tmp = -1\n",
    "#                 for indd, roww in sdd.iterrows():\n",
    "#                     if row.pred == 1:\n",
    "#                         if roww.close > close_previous:\n",
    "#                             if tmp < (roww.close - close_previous):\n",
    "#                                 tmp = roww.close - close_previous\n",
    "#                             check = 1\n",
    "#                     else:\n",
    "#                         if roww.close < close_previous:\n",
    "#                             if tmp < (close_previous - roww.close):\n",
    "#                                 tmp = close_previous - roww.close\n",
    "#                             check = 1\n",
    "#                 if check == 1:\n",
    "#                     acc = pd.concat([acc, pd.DataFrame(\n",
    "#                         [[dt_orig.strftime('%Y-%m-%d'), check, tmp]], columns =  ['date','accuracy', 'potential'])]\n",
    "#                                    )\n",
    "#                 else : \n",
    "#                     acc = pd.concat([acc, pd.DataFrame(\n",
    "#                         [[dt_orig.strftime('%Y-%m-%d'), check, tmp]], columns =  ['date','accuracy', 'potential'])]\n",
    "#                                    )\n",
    "\n",
    "#         acc.to_csv(\"results/CMG/accuracy/\"+i+\".csv\")           \n",
    "#         print(i,acc['accuracy'].mean())\n",
    "        clear_output(wait=True)\n",
    "        gc.collect()\n",
    "    except Exception as e:\n",
    "        print(\"❌ An error occurred:\")\n",
    "        traceback.print_exc()\n",
    "        gc.collect()\n",
    "    \n",
    "for i in df_main.symbol:\n",
    "# for i in df_main[ df_main [df_main.symbol == 'NSEI'].index[0]:].symbol:\n",
    "    print(i)\n",
    "    run_simulation(i, 3, 400)\n",
    "    break\n",
    "# Force garbage collection\n",
    "\n",
    "# with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "#     results = list(executor.map(run_simulation, list(df_main['symbol'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "195b85c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 6)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5109d65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.72313084112147\n",
      "CPU times: user 1.01 s, sys: 92.5 ms, total: 1.1 s\n",
      "Wall time: 2.85 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get the average accuracy\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Specify the folder path\n",
    "folder_path = '../cmg_results/CMG/accuracy/'  # Replace with your folder path\n",
    "li = []\n",
    "# Get all CSV files in the folder\n",
    "csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "for file in csv_files:\n",
    "    tmp = pd.read_csv(file, index_col = [0])\n",
    "#     li.append(tmp.loc[\"accuracy\", \"f1-score\"])\n",
    "    li.append(tmp[\"accuracy\"].mean())\n",
    "    \n",
    "print(sum(li)/len(li)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32cbc557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87057ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbdd4e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54608e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595915b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef1ce5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bfbfc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d4a64b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542e8666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431d47b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forex",
   "language": "python",
   "name": "forex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
